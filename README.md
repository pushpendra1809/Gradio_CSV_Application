# Gradio_CSV_Application
# Gradio-based CSV Question Answering and Visualization Application

## Overview

This project is a Gradio-based application that allows users to upload a CSV file, ask questions (including numerical queries) about its contents, and receive answers generated by a local Large Language Model (LLM). The application also supports graph plotting, with all visualizations displayed within the Gradio interface.

## Features

- **CSV File Handling**: Users can upload CSV files, and the application validates the file format while handling parsing errors gracefully.
- **Question Answering**: Users can input textual or numerical questions related to the CSV data. Answers are generated using a local LLM via the Ollama framework, integrated with Pydantic AI for structured query processing.
- **Graph Plotting**: The application supports generating graphs based on the CSV data, with all visualizations embedded within the Gradio interface.

## Functional Requirements

### 1. CSV File Handling
- **File Upload**: Users can upload CSV files (max size: 25MB).
- **Validation**: The application validates the CSV file format and handles parsing errors gracefully.

### 2. Question Answering
- **User Interaction**: Users can input questions related to the CSV data.
- **LLM Integration**: Answers are generated using a local LLM (recommended model: Llama 3.1 8B or smaller) via the Ollama framework. The LLM agent is implemented using Pydantic AI for structured and efficient query processing.
- **Quantization**: The developer selects an optimal quantization level to balance performance and resource usage.

### 3. Graph Plotting and Visualization
- **Graph Generation**: The application supports generating graphs based on the CSV data.
- **Embedded Visualization**: All plotted graphs are displayed within the Gradio app interface.

## Technical Requirements

### 1. Frameworks and Libraries
- **Gradio**: The frontend is built using the Gradio framework.
- **Pydantic AI**: Used for structured and reliable AI-powered query processing.
- **Ollama**: Serves as the LLM backend to run models locally, ensuring quick response times.

### 2. Model Specification
- **Recommended Model**: Llama 3.1 8B or a smaller model.
- **Quantization Level**: The developer selects an optimal quantization level for local deployment performance.

### 3. Application Architecture
- **Modularity**: The system is modular, with clear separation between:
  - File handling
  - Query processing using Pydantic AI
  - LLM integration with Ollama
  - Graph plotting
- **Error Handling**: Robust error handling is implemented for:
  - CSV parsing errors
  - User input validation
  - LLM processing failures
### 4. IDE: Pycharm

## Sample CSV Data
The application is tested with the following sample CSV data:
- **Housing Price Dataset**: A dataset from Kaggle containing both numerical and string data.

